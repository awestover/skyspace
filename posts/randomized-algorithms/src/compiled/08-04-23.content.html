<p>In this short post I will comment about what I learned from reading William Kuszmaul’s “Train Tracks with Gaps” paper in FUN 2021.</p>
<h1 id="general-notes">general notes</h1>
<ol type="1">
<li>The paper is indeed pretty fun. Check it out yourself.</li>
<li>There is a lot of clever union bounding that goes on</li>
<li>“do each thing separately independently randomly with some probability p” is generally a pretty good strategy</li>
<li>Lower bound proof: McDiarmid’s Inequality is a generalization of Chernoff bound. If you have a random variable <span class="math inline">\(F\)</span> that depends on a bunch of random variables <span class="math inline">\(A_1,A_2,\ldots, A_n\)</span>, but alterning any one particular <span class="math inline">\(A_i\)</span> has a bounded effect on <span class="math inline">\(F\)</span> (“Lipshitz Condition”) then we can get an exponential concentration bound on <span class="math inline">\(F\)</span>.</li>
<li>Often useful to consider the “counts” random variabe even when you just care about whether or not something is <span class="math inline">\(0\)</span>. Because you can analyze expectation and variance of a counts random variable.</li>
<li>Now we discuss the three algorithms for building track. Like we know from a probabilistic argument that a valid track exists, and now we want an <strong>efficient</strong> algorithm to build it. Of course an innefficient trivial brute force algorithm exists. But this is not so interesting in exponentially-sized search spaces.</li>
</ol>
<h1 id="zeroth-algorithm">Zeroth Algorithm:</h1>
<p>Actually the existence proof via prob method with alterations already gives a trivial algorithm that works. But we are going to talk about some other algorithms that work and give same guarantees for educational purposes.</p>
<h1 id="first-algorithm">First Algorithm:</h1>
<p>“Method of conditional probabilities” The story is you have random variables <span class="math inline">\(X_1,\ldots, X_\ell\)</span> and a function <span class="math inline">\(F\)</span> such that <span class="math inline">\(\mathop{\mathrm{\mathbb{E}}}[F(X_1,\ldots, X_\ell)]\le R\)</span>. And you want to find settings <span class="math inline">\(x_1,\ldots, x_\ell\)</span> which make <span class="math inline">\(F(x_1,\ldots, x_\ell)\le R\)</span>. Then we can do this itteratively.</p>
<p>Our <span class="math inline">\(X_i\)</span> are boolean random variables whether the track exists that occur with probability Our <span class="math inline">\(F\)</span> is the amount of train track required if you take the <span class="math inline">\(X_i\)</span>’s which are turned on and then add stuff to fix the places that are broken</p>
<p>In particular, if we have selected <span class="math inline">\(x_1,\ldots, x_k\)</span> so that <span class="math display">\[\mathop{\mathrm{\mathbb{E}}}[F(x_1,\ldots, x_k, X_{k+1},\ldots, X_\ell)]\le R\]</span> then there must be <span class="math inline">\(x_{k+1}\)</span> which makes <span class="math display">\[\mathop{\mathrm{\mathbb{E}}}[F(x_1,\ldots, x_k, x_{k+1},X_{k+2}\ldots, X_\ell)]\le R\]</span></p>
<p>So, the only trick is making it easy to compute this expectation.</p>
<p>and you can do this for the train thing.</p>
<h1 id="second-algorithm">Second Algorithm:</h1>
<p>First we review the Lovasz Local Lemma: you have independent random variables <span class="math inline">\(X_1,\ldots, X_s\)</span> Then you have events <span class="math inline">\(E_1,\ldots, E_m\)</span> such that each <span class="math inline">\(E_i\)</span> is determined by some set of <span class="math inline">\(X_j\)</span>’s. If each event is unlikely, i.e. there exists small <span class="math inline">\(p\)</span> such <span class="math inline">\(\Pr[E_i]\le p\)</span> for all <span class="math inline">\(i\)</span>, and the dependencies between the events are low, i.e., there exists small <span class="math inline">\(d\in \mathbb{N}\)</span> such that each <span class="math inline">\(E_i\)</span> depends on some shared <span class="math inline">\(X_j\)</span> with at most <span class="math inline">\(d\)</span> other <span class="math inline">\(E_{i&#39;}\)</span>’s, in particular if we have <span class="math inline">\(pd\le \frac{1}{e}\)</span> then there is positive probability that none of <span class="math inline">\(E_1,\ldots, E_m\)</span> occur.</p>
<p>Algorithmic Lovasz Local Lemma is an efficient algorithm to find this. “Fix-it algorithm”: Sample the <span class="math inline">\(X_i\)</span>’s randomly. So long as one of the events <span class="math inline">\(E_j\)</span> happened resample its <span class="math inline">\(X_i\)</span>’s. The expected number of resamplings that need to happen is at most <span class="math inline">\(\frac{n}{d}\)</span></p>
<h1 id="third-algorithm">Third Algorithm:</h1>
<p>Min-hashing</p>
<p>You have a collection of (not-necessarily-disjoint) sets and want to sample an element from each of them.</p>
<p>How you do it: assign each element a random value in <span class="math inline">\((0,1)\)</span>. For each set take the element with lowest value.</p>
<p>It turns out that this covers all the sets while using a very small number of elements.</p>
<h1 id="summary">Summary</h1>
<p>Dang these are some neat ideas! I bet they are applicable to lots of problems.</p>
