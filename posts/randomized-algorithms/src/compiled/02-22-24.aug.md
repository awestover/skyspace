\renewcommand{\O}{\mathcal{O}}
\newcommand{\tilo}{\widetilde{\O}}
\newcommand{\nil}{\varnothing}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}
\newcommand{\legendre}[2]{\genfrac{(}{)}{}{}{#1}{#2}}
\newcommand{\EXP}{\mathsf{EXP}}
\newcommand{\PSPACE}{\mathsf{PSPACE}}
\newcommand{\NEXP}{\mathsf{NEXP}}
\newcommand{\PP}{\mathsf{P}}
\newcommand{\NP}{\mathsf{NP}}
\newcommand{\erdos}{Erd\H{o}s}
\newcommand{\pmo}{\set{-1,1}}
\newcommand{\zo}{\set{0,1}}


\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


<div class="thm envbox">**Theorem.**
Effron-Stein Inequailty.

$$ \Var[F] \le \frac{1}{2} \sum_{i=1}^{n} \E_{X_1,X_2,\ldots, X_n, X_i'}(F(X_1,\ldots, X_i, \ldots, X_n) -F(X_1,\ldots, X_i', \ldots, X_n))^2 $$ 
</div>
<div class="pf envbox">**Proof.**
We can prove a weaker version (just drop the $1/2$) by induction
on $n$. 
The formula reduces to Jensen's Inequality if you set $n=1$.

Now, lets try to do $n=2$. The extension from $n$ to  $n+1$
should just follow the same pattern.

It turns out that you can prove the inequality **point-wise**.
I.e., fix $Y$ and condition on it. You do some algebra and apply
Cauchy-Shwarz and/or Jensens and it works.
</div>

<div class="ex envbox">**Example.**
You can use this to show that MAXCUT of a random graph
concentrates. 
weird rmk: we don't actually know what it concentrates around!
Of course it is $\Theta(n)$ [the setting is  $G(n,d/n)$ for
constant $d$ ]. But the precise value is an open question.
</div>

<div class="thm envbox">**Theorem.**
Azuma-Hoeffding Bound

Suppose you have a martingale with almost surely bounded
differences, namely, $|X_k-X_{k-1}|\le d_k$.

Then we have 
 $$ \Pr[|X-\E X| > t] \le 2\exp( \frac{-t^2}{2\sum d_k^2}.  ) $$ 

 In the special case where the increment bounds $d_k$ are all the same this is 

 $$ \Pr[|X-\E X| > t] \le 2\exp( \frac{-t^2}{2nd^2}.  ) $$ 

</div>
<div class="pf envbox">**Proof.**

As always we do the Chernoff trick: Markov the exponential MGF.
We bound the exponential MGF by some convexity argument that
works using the boundedness of the increments.

</div>

<div class="cor envbox">**Corollary.**
McDiarmid's Inequality

Suppose $F(X_1,\ldots, X_n)$, where $X_1,\ldots, $ are iid, is
lipschitz: i.e., if you mess with a few coordinates you don't
mess with it v much.

Then we also get a chernoff-like tail bound concentration thing.

For instance, if messing with one variable can change the value
by at most $d$ then we have

$$ \Pr[|F(X_1,\ldots, X_n) - \E F| > t] \le 2\exp( \frac{-2t^2}{nd^2} ). $$ 

</div>
<div class="pf envbox">**Proof.**
This is actually the special case of Azuma Hoeffding applied to
the Doob Martingale defined  by sequentially revealing more
information about the variables feeding into $F$.
</div>

