<p>Imagnie I have a postgres database with ~3000 columns storing boolean “flags” for ~1 billion rows. The “on” flag (1) is quite sparse, say occuring less than 0.1% of the time in each column. Now, I want to execute a query of the form “select all rows with flags 1,2, 5 and 101” 1. How much space would it take to store such a database? That is, will postgres by default do any sort of compression because of how sparse the “on” flag is? 2. Does postgres by default make queries of this form “fast”? If not, are there well known postgres extensions for this type of query? If not I might write my own, but just want to know if this has already been done.</p>
<p>This blog post contains my thoughts on this. First, we parameterize some things:</p>
<div class="defn envbox">
<p><strong>Definition.</strong> - <span class="math inline">\(n\)</span> number of rows - <span class="math inline">\(L\)</span> number of columns (categories) - <span class="math inline">\(\delta\)</span> density in each column - <span class="math inline">\(q\)</span> query size - <span class="math inline">\(w\)</span> word size (we can do <span class="math inline">\(O(1)\)</span>-time operations on words) - <span class="math inline">\(p\)</span> number of processors available (we can do <span class="math inline">\(p\)</span> units of work per a single unit of time.)</p>
</div>
<h2 id="storage">Storage</h2>
<p><strong>Way 1:</strong> Have <span class="math inline">\(n\)</span> rows, with <span class="math inline">\(L\)</span> bits, and thus <span class="math inline">\(L/w\)</span> words, per row. Space: <span class="math inline">\(\frac{nL}{w}\)</span>.</p>
<p><strong>Way 2:</strong> For each of the <span class="math inline">\(L\)</span> categories, store the row-indices of the <span class="math inline">\(\sim \delta n\)</span> rows in this category. Overall this requires <span class="math display">\[\sim \delta n L \frac{\log n}{w}\]</span> space.</p>
<p>Comparison: as long as the density (or should I say sparsity?) is <span class="math inline">\(\delta &lt; \frac{1}{\log n}\)</span> way 2 is the clear winner here. For instance, if <span class="math inline">\(n\sim 10^{9}\)</span>, the required sparsity for way 2 to win is about <span class="math inline">\(\delta &lt; \frac{1}{30}\)</span>.</p>
<h2 id="query">Query</h2>
<p>With “way 1” the query can be handled by partitioning the rows into <span class="math inline">\(p\)</span> groups and summing the bit-masked numbers.</p>
<p>This requires time: <span class="math display">\[\frac{n}{p} \cdot \frac{L}{w}.\]</span></p>
<p>With “way 2” the query can be handled by partitioning the rows again into <span class="math inline">\(p\)</span> groups. But this time we should be careful to “randomly” partition the rows somehow. It probably doesn’t need to be completely a random partition, because that sounds expensive. Should suffice to, e.g., assign each row to that processor iid with probability <span class="math inline">\(\frac{1}{p}\)</span>. Note: this must be done before the query. You modify the storage structure so that each processor has a list of <span class="math inline">\(\sim n\delta/p\)</span> elements per each category. Now, each processor can do its stuff in time <span class="math display">\[\approx  q n\delta / p \frac{\log n}{w}\]</span></p>
<p>So as long as <span class="math display">\[\frac{q}{L / \log n} &lt; \frac{1}{\delta}\]</span> way 2 is better. Which kind of makes sense.</p>
