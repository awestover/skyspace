\renewcommand{\O}{\mathcal{O}}
\newcommand{\tilo}{\widetilde{\O}}
\newcommand{\nil}{\varnothing}
\newcommand{\one}{\mathbb{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}
\newcommand{\legendre}[2]{\genfrac{(}{)}{}{}{#1}{#2}}
\newcommand{\EXP}{\mathsf{EXP}}
\newcommand{\PSPACE}{\mathsf{PSPACE}}
\newcommand{\NEXP}{\mathsf{NEXP}}
\newcommand{\PP}{\mathsf{P}}
\newcommand{\NP}{\mathsf{NP}}
\newcommand{\erdos}{Erd\H{o}s}
\newcommand{\pmo}{\set{-1,1}}
\newcommand{\zo}{\set{0,1}}


\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


brevity is ~~the soul of wit~~ fast ig

# FKG

FKG says if $F,G$ are **increasing** functions of independent
random variables $X_1,\ldots,
X_n$ then we have

$$ \E[FG] \ge \E[F]\E[G]. $$ 

For instance, suppose that $X_1,\ldots, X_n$ are whether edges
exist in erdos renyi graph.
And $F$ was the indicator of "existence of a 4-clique" and $G$
was the indicator of "graph is connected".
Then the inequality reads

"Probability of being connected and containing a 4-clique is
larger than if you pretended like these were independent events."

Or if $F$ was "number of triangles in graph" then you have
"expected number of triangles conditional on having a 4-clique is
larger than expected number of triangles with no conditioning."

This makes a lot of sense: positively correlated things should
help each other out.

# janson 

[yufei's notes](https://ocw.mit.edu/courses/18-226-probabilistic-method-in-combinatorics-fall-2020/mit18_226f20_lec15-16.pdf)

Let $R$ be a random subset of $[n]$ generated somehow.
Let $A_i$ be the event that $S_i\subseteq R$. 
Let $X = \sum_i \one_{A_i}$.
Define
$$ \Delta = \sum_{A\not\perp B} \E[I_A I_B] $$ 

Then you get a tail bound
$$ \Pr[X\le \E X - t]\le \exp(\frac{-t^2}{2\Delta}). $$ 

prototypical example: analyze number of triangles in graph.

<div class="ex envbox">**Example.**

Let $R$ be, you choose some edges (edros renyi graph).
Index the $\binom{n}{3}$ potential triangles in the graph.
Define event $A_i$ to be the event that all edges in the $i$-th triangle exist.
Then $\Delta = \Theta(p^3 n^3 + p^{5}\binom{n}{4}).$

So you can say some nice things about the triangle counts in your
graph.

Like this is pretty neat. Usually if you were just
second-moment-methoding your probabiltiy bounds start kicking in
at the same place, but they are *way weaker*.

</div>

# martingales

After working with martingales for a bit on the pset I'm not sure
I have a great intuition for them. But the vague idea that I have
is, you wanna find some kind of invariant. 
Feels like hoping to transform your RV into at least a
supermartingale / submartingale seems like a pretty general and
reasonable thing to do. Probably do more examples to get a better
sense for when it's useful.

