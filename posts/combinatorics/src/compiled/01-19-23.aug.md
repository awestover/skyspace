\newcommand{\one}{\mathbbm{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


We aim to bound
$$\sum_{i\leq \epsilon n} \binom{n}{i}$$

The answer, roughly speaking, should be $c(\epsilon)^n$ for some
$1 \leq c(\epsilon) \leq 2$.

We can think of this probabilistically as being related to the
following problem:
Let $X_1,\ldots, X_n$ be independent random variables with
probabilty $1/2$ of occuring. These random variables represent
taking a subset of $[n]$.
Then we are concerned with 
$$\Pr[\sum X_i \leq \epsilon n].$$

In particular, 
$$\Pr[\sum X_i \leq \epsilon n] = 2^{-n} \sum_{i\leq \epsilon
n}\binom{n}{i}.$$

Applying one form of the Chernoff bound listed on wikipedia 
we can bound our sum by

$$\Pr[\sum X_i \leq \epsilon n] \leq
\left(\frac{2}{\sqrt{e}}\left(\frac{e}{2\epsilon}\right)^\epsilon\right)^n.$$

<div class="rmk envbox">**Remark.**
This is basically tight for $\epsilon \approx 1/2$, and pretty
decent for e.g. $\epsilon\in (.1,.5)$.
</div>

![ratio](src/images/combo_dude.png)


```python
import matplotlib.pyplot as plt
from math import comb, sqrt, log2, e

n = 100
idxs = list(range(n//10,n//2))
actual = [1 for k in range(n)]
log_estimate = []
for k in range(1,n):
  actual[k] = actual[k-1] + comb(n, k)

  eps = k/n
  log_estimate.append( log2(2/sqrt(e)) + eps*log2(e/(2*eps)) ) 

log_actual = [log2(a)/n for a in actual]
ratio = [log_estimate[i]/log_actual[i] for i in idxs]

plt.plot(idxs, ratio)
plt.show()

```

**UPDATE**
OK, apparently the answer can be expressed in terms of $H$,
the binary entropy function:
$$\Pr[\sum X_i \leq \epsilon n]  \approx 2^{H(\epsilon)n}.$$

![entropy](src/images/entropy_curve.png)

