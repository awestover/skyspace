\renewcommand{\O}{\mathcal{O}}
\newcommand{\tilo}{\widetilde{\O}}
\newcommand{\nil}{\varnothing}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}
\newcommand{\legendre}[2]{\genfrac{(}{)}{}{}{#1}{#2}}
\newcommand{\EXP}{\mathsf{EXP}}
\newcommand{\PSPACE}{\mathsf{PSPACE}}
\newcommand{\NEXP}{\mathsf{NEXP}}
\newcommand{\PP}{\mathsf{P}}
\newcommand{\NP}{\mathsf{NP}}
\newcommand{\erdos}{Erd\H{o}s}
\newcommand{\pmo}{\set{-1,1}}
\newcommand{\zo}{\set{0,1}}


\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


> Shatar: Hey JJ what's cooking?\
> JJ: At this point in time I personally am not cooking anything.
However I intuit that your meaning is more metaphorical, similar
to the phrase "what's up". Thanks much for your inquiry. Lately
I've been studying boolean functions, specifically Chapter 2 of
O'Donnel's book which discusses social choice theory.\
> Shatar: Oh no way! Wait I actually love that topic. You know
the best voting rule?\
> JJ: The answer to your question depends on the metric you use
to evaluate the voting rule.\
> Shatar: The metric is obviously *coolness* and *makes the most
sense*. Anyways, here's probably the best voting rule:

> You have a bunch of little groups, called "tribes" of people.
Each of these groups is debating whether or not to go feed this
cat that they saw sitting out in the rain.
Within each tribe here is what happens: either, everyone
unanimously votes in favor of helping the cat, in which case the
tribe will go help the cat / elect the cat as president.
However, if anyone in the group opposes the decision to help the
cat the group will just argue about philosophy and ignore the
cat. The cat is elected as president / saved off the side of the
road iff at least one tribe decides to help it.

> JJ: Ah yes this is called the "tribes" voting rule.\
> Shatar: It's so logical, right! Anyways, one reason why it's
cool is that it apparently gives essentially the smallest
possible influence to each person.

Recall: 

<div class="defn envbox">**Definition.**
$$I_i[f] = \Pr_x[f(x)\neq f(x\cdot e_i)] = \Theta(1)\cdot
||\partial_i f||_2^{2}.$$
</div>

<div class="rmk envbox">**Remark.**
In the following theorem it is actually important what base the
logs are in the definition of the size of the tribes! So I've written it explicitly.

I feel like there are a couple times lately when log bases are
important. E.g., $\ln (1+x)\le x$. Whatever.
</div>

<div class="thm envbox">**Theorem.**
If you set $w = \log_2 (n/\log_2 n)$ and $s = n/w$ then 
TRIBES with $s$ size-$w$ tribes is a balanced function that gives
each coordinate influence only 
$$\O(1)\cdot \frac{\log n}{n}$$
</div>
<div class="pf envbox">**Proof.**

First, we show it's balanced.
The chance that no-one helps the cat is 
$$(1-2^{-w})^{s} \approx (1 - \frac{\log n}{n})^{n/\log n} \approx 1/2.$$

Next, we compute the influence of each variable. It is:
$$2(1-2^{-w})^{s-1}2^{-(w-1)}\approx 1/2^{w} = \frac{\log n}{n}.$$

</div>

> JJ: That's pretty interesting. Thanks for telling me about it!\
> Shatar: What's your vote?

<img src='../../images/cat.png' width='25%'>

---

Now I just review several things from the first two weeks of
class. Reference: Dor Minzer's lecture notes.

<div class="rmk envbox">**Remark.**
Random resetrictions are important.
Here are some of their crucial properties:

For any $S\subseteq \hat{J}$,
$$\widehat{f_{J\to y}}(S) = \sum_{T\subseteq J} \hat{f}(S\sqcup T) \chi_T(y).$$
That is, to get the fourier coefficient at $S$ you smear the
fourier coefficients at $S\sqcup T$ for all $T$ in the subset of
indices that we fixed. Specifically when doing this smearing it
is weighted based on the $T$ character evaluated at the value we
restrict to.

One consequence of this fact is that:
for any $S\subseteq \hat{J}$,
$$\E_y \widehat{f}_{J\to y} (S) = \hat{f}(S).$$
That is, the expectation of the character is just the character.

Finally, we have  the following equation for the "second moment":
$$\E_y \widehat{f_{J\to y}}(S)^2 = \sum_{T\subseteq \overline{J}} \hat{f}(S\sqcup T)^2.$$


Intuition is 

- decrease degree of polynomial
- make some functions like AND drop out to constants
- in general simplify your function

</div>

<div class="lem envbox">**Lemma.**
Given $T\subseteq J \subseteq [n]$ and membership queries to
boolean function $f$ you can, with good probability, approximate
$$\sum_{S: S\cap J = T} \hat{f}(S)^2.$$
</div>

<div class="defn envbox">**Definition.**
Say that a function $f$ is $t$-sparse if its fourier spectrum $\hat{f}$ is supported
on at most $t$ characters.

Say $f,g$ are $\eps$-close if $||f-g||_2^{2}\le \eps$.
</div>

<div class="thm envbox">**Theorem.**
Given a function which is $\eps$-close to being $t$-sparse, we
can, with good probability, efficiently learn a function $g$ that is close to $f$.
</div>
<div class="pf envbox">**Proof.**
Suffices to focus  on heavy coefficients.

Key step: using the key lemma we can estimate, e.g., 
$$\sum_{S: S\cap \set{1}=\set{1}} \hat{f}(S)^2.$$
If it's big it means that there must be some heavy fourier
coefficient containing $1$.
We build a tree and find everything.

</div>
