<blockquote>
<p>Shatar: Hey JJ what’s cooking?<br />
JJ: At this point in time I personally am not cooking anything. However I intuit that your meaning is more metaphorical, similar to the phrase “what’s up”. Thanks much for your inquiry. Lately I’ve been studying boolean functions, specifically Chapter 2 of O’Donnel’s book which discusses social choice theory.<br />
Shatar: Oh no way! Wait I actually love that topic. You know the best voting rule?<br />
JJ: The answer to your question depends on the metric you use to evaluate the voting rule.<br />
Shatar: The metric is obviously <em>coolness</em> and <em>makes the most sense</em>. Anyways, here’s probably the best voting rule:</p>
</blockquote>
<blockquote>
<p>You have a bunch of little groups, called “tribes” of people. Each of these groups is debating whether or not to go feed this cat that they saw sitting out in the rain. Within each tribe here is what happens: either, everyone unanimously votes in favor of helping the cat, in which case the tribe will go help the cat / elect the cat as president. However, if anyone in the group opposes the decision to help the cat the group will just argue about philosophy and ignore the cat. The cat is elected as president / saved off the side of the road iff at least one tribe decides to help it.</p>
</blockquote>
<blockquote>
<p>JJ: Ah yes this is called the “tribes” voting rule.<br />
Shatar: It’s so logical, right! Anyways, one reason why it’s cool is that it apparently gives essentially the smallest possible influence to each person.</p>
</blockquote>
<p>Recall:</p>
<div class="defn envbox">
<p><strong>Definition.</strong> <span class="math display">\[I_i[f] = \Pr_x[f(x)\neq f(x\cdot e_i)] = \Theta(1)\cdot
||\partial_i f||_2^{2}.\]</span></p>
</div>
<div class="rmk envbox">
<p><strong>Remark.</strong> In the following theorem it is actually important what base the logs are in the definition of the size of the tribes! So I’ve written it explicitly.</p>
<p>I feel like there are a couple times lately when log bases are important. E.g., <span class="math inline">\(\ln (1+x)\le x\)</span>. Whatever.</p>
</div>
<div class="thm envbox">
<p><strong>Theorem.</strong> If you set <span class="math inline">\(w = \log_2 (n/\log_2 n)\)</span> and <span class="math inline">\(s = n/w\)</span> then TRIBES with <span class="math inline">\(s\)</span> size-<span class="math inline">\(w\)</span> tribes is a balanced function that gives each coordinate influence only <span class="math display">\[\mathcal{O}(1)\cdot \frac{\log n}{n}\]</span></p>
</div>
<div class="pf envbox">
<p><strong>Proof.</strong></p>
<p>First, we show it’s balanced. The chance that no-one helps the cat is <span class="math display">\[(1-2^{-w})^{s} \approx (1 - \frac{\log n}{n})^{n/\log n} \approx 1/2.\]</span></p>
<p>Next, we compute the influence of each variable. It is: <span class="math display">\[2(1-2^{-w})^{s-1}2^{-(w-1)}\approx 1/2^{w} = \frac{\log n}{n}.\]</span></p>
</div>
<blockquote>
<p>JJ: That’s pretty interesting. Thanks for telling me about it!<br />
Shatar: What’s your vote?</p>
</blockquote>
<p><img src='../../images/cat.png' width='25%'></p>
<hr />
<p>Now I just review several things from the first two weeks of class. Reference: Dor Minzer’s lecture notes.</p>
<div class="rmk envbox">
<p><strong>Remark.</strong> Random resetrictions are important. Here are some of their crucial properties:</p>
<p>For any <span class="math inline">\(S\subseteq \hat{J}\)</span>, <span class="math display">\[\widehat{f_{J\to y}}(S) = \sum_{T\subseteq J} \hat{f}(S\sqcup T) \chi_T(y).\]</span> That is, to get the fourier coefficient at <span class="math inline">\(S\)</span> you smear the fourier coefficients at <span class="math inline">\(S\sqcup T\)</span> for all <span class="math inline">\(T\)</span> in the subset of indices that we fixed. Specifically when doing this smearing it is weighted based on the <span class="math inline">\(T\)</span> character evaluated at the value we restrict to.</p>
<p>One consequence of this fact is that: for any <span class="math inline">\(S\subseteq \hat{J}\)</span>, <span class="math display">\[\mathop{\mathrm{\mathbb{E}}}_y \widehat{f}_{J\to y} (S) = \hat{f}(S).\]</span> That is, the expectation of the character is just the character.</p>
<p>Finally, we have the following equation for the “second moment”: <span class="math display">\[\mathop{\mathrm{\mathbb{E}}}_y \widehat{f_{J\to y}}(S)^2 = \sum_{T\subseteq \overline{J}} \hat{f}(S\sqcup T)^2.\]</span></p>
<p>Intuition is</p>
<ul>
<li>decrease degree of polynomial</li>
<li>make some functions like AND drop out to constants</li>
<li>in general simplify your function</li>
</ul>
</div>
<div class="lem envbox">
<p><strong>Lemma.</strong> Given <span class="math inline">\(T\subseteq J \subseteq [n]\)</span> and membership queries to boolean function <span class="math inline">\(f\)</span> you can, with good probability, approximate <span class="math display">\[\sum_{S: S\cap J = T} \hat{f}(S)^2.\]</span></p>
</div>
<div class="defn envbox">
<p><strong>Definition.</strong> Say that a function <span class="math inline">\(f\)</span> is <span class="math inline">\(t\)</span>-sparse if its fourier spectrum <span class="math inline">\(\hat{f}\)</span> is supported on at most <span class="math inline">\(t\)</span> characters.</p>
<p>Say <span class="math inline">\(f,g\)</span> are <span class="math inline">\(\varepsilon\)</span>-close if <span class="math inline">\(||f-g||_2^{2}\le \varepsilon\)</span>.</p>
</div>
<div class="thm envbox">
<p><strong>Theorem.</strong> Given a function which is <span class="math inline">\(\varepsilon\)</span>-close to being <span class="math inline">\(t\)</span>-sparse, we can, with good probability, efficiently learn a function <span class="math inline">\(g\)</span> that is close to <span class="math inline">\(f\)</span>.</p>
</div>
<div class="pf envbox">
<p><strong>Proof.</strong> Suffices to focus on heavy coefficients.</p>
<p>Key step: using the key lemma we can estimate, e.g., <span class="math display">\[\sum_{S: S\cap \left\{ 1\right\}=\left\{ 1\right\}} \hat{f}(S)^2.\]</span> If it’s big it means that there must be some heavy fourier coefficient containing <span class="math inline">\(1\)</span>. We build a tree and find everything.</p>
</div>
