\newcommand{\one}{\mathbbm{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


One of the most basic facts of linear algebra is that whenever
you have $n+1$ vectors in an $n$-dimensional vectors space, there
must be one of your vectors which can be expressed as a linear
combination of the other vectors. This seemingly simple fact can
prove some really interesting combinatorial statements. 


Not sure if this one counts:
<div class="thm envbox">**Theorem.**
Let $x_1,x_2,\ldots, x_k$ be $k$ linearly-independent vectors in,
say, $\F_2^{n}$. Then there is a vector $y$ in the span of
$x_1,x_2,\ldots, x_k$ with at least $k$ non-zero entries. 
</div>
<div class="pf envbox">**Proof.**
Write the $x_1,x_2,\ldots, x_k$ as an $n \times k$ matrix.
It must have a $k\times k$ invertible submatrix. Why? Because
just delete rows one at a time.
</div>

<div class="thm envbox">**Theorem.**
The vertices of any graph $G$ can be partitioned into two parts $V_1,V_2$
such that $G[V_1], G[V_2]$ have all vertices having even degree. 
</div>
<div class="pf envbox">**Proof.**

Let $x_i\in \F_2$ be a binary variable indicating which part to
put vertex $i$ in. 
Then write some equations:
For $i$ with odd degree write:
$$x_i + \sum_{j\in N(i)}x_j = 1$$ 
For $i$ with even degree write:
$$\sum_{j\in N(i)}x_j = 0.$$ 

claim: It must have a solution.
proof: 
you might be highly skeptical: "thaht matrix needn't be
invertible." well fine, but we don't need it to be invertible,
we just need this very specific vector to be in its image. 

Letting $D$ be a diagonal matrix with $D_{i,i}$ being the degree
of vertex  $i$, and letting $d$ be a vector with $d_i$ being the
degree of vertex $i$, we have:

$$ d \in Im (A+D)$$
This is equiv to 
$$ (Im (A+D)) ^{\perp} \subseteq d^{\perp} .$$

Take $x\in( Im(A+D))^{\perp}$.
Then for all $i$, 
$$\sum_j x_j (a_{i,j}+d_{i,j}) = 0,$$
so 
$$\sum_j x_j a_{i,j} = x_i d_i.$$
Then 
$$\sum_i x_i \sum_j x_j a_{i,j} = \sum_i x_i^{2}d_i $$

But then of course 
$$\sum_i x_i d_i = \sum_{i<j} (a_{i,j}+a_{j,i})x_{i}x_j + \sum_{i}x_i a_{i,i} = 0.$$
That is, $x\perp d$ as desired.

Intuitively what just happened? 
"weighted sums are kind of powerful" is the extent to which I
understand this so far. 


</div>
