\newcommand{\one}{\mathbbm{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


Throughout the post I will generally omit log factors. In some
places I have written $n^{\omega+o(1)}$, but really this should
probably be written in most places. 

# lecture 1

Lots of matrix problems, e.g., matrix inversion, are equivalent
to MM or at least closely related to MM. 

# lecture 2

4-russians: shave some logs by lookup  tables of size $\log n$

### triangle detection to triangle finding
Randomly split vertices in half. Run detection on both halves. 
Have to try $O(1)$ times in expectation. 
The run time would be, in expectation, 
$$\sum T(n/2^{i})\le T(n) 2\sum (1-\eps)^{i}\le T(n)2\eps^{-1}.$$

This can be de-randomized as follows: 
split the set into $6$ equal sized parts. There must be some set
of $3$ of them that contains a triangle. Every set of $3$ of them
has size $n/2.$


### using triangle detection to do BMM

BMM is defined as 
$$(AB)[i,j] = \bigvee_k (A[i,k]\land B[k,j]).$$

<div class="rmk envbox">**Remark.**
You can do BMM via fast MM techniques in $n^{\omega}$ time.
</div>

<div class="thm envbox">**Theorem.**
You can also do BMM via triangle-detection.
This is asymptotically slower, but kind of nicer because it
doesn't rely on crazy impractical tensor-decomposition stuff with
ridiculous constant factors. 
More precisely, 
if there is a $D(n)$ time algorithm for triangle detection then
there is an $n^{2}D(n^{1/3})$ time algorithm for BMM.
Using BMM to do triangle detection is trivial. 
Thus, BMM and triangle detection are "sub-cubic equivalent".
</div>
<div class="pf envbox">**Proof.**
Make a tri-partite graph $X,Y,Z$.
Put all the edges between $X,Z$. 
Put edges between $i\in X,j\in Y$ if $A[i,j]=1$
and edges between  $j\in Y, k\in Z$ if  $B[j,k]=1$.

Split each of $X,Y,Z$ into parts of size $t$.
For each tripple of little parts eat all the triangles in it. 

Run time: 
$$t^3 D(n/t) + n^{2} D(n/t).$$
Set $t=n^{2/3}$.
</div>


### more notes

<div class="thm envbox">**Theorem.**
transitive closure is sub-cubicly equivalent to BMM
</div>
<div class="pf envbox">**Proof.**
One direction is like this:

![ink_img003](src/images/ink_img003.png)

Other direction is like this:
compute SCC, topo-sort the SCC DAG
Trans-closure of DAG can be done with MM.
In the trans-closure, $A+I$ is upper-triangular.

</div>

<div class="thm envbox">**Theorem.**
You can compute strongly connected components in $n^{2}$ time.
</div>

# lecture 3: intro to APSP

APSP: 
Given a weighted (possibly directed) graph, compute $d(u,v)$ for
all vertices $u,v$.

Best algorithm as of 2021 (Ryan Williams):
$n^{3} / 2^{\Omega(\sqrt{\log n})}$.

Observation: $d(u,v)$ is the smallest  $k$ for which
$A^{k}[u,v]\neq 0$. 
This is great if the graph has small diameter. 
For far away vertices we need another technique

### hitting set algorithm

<div class="lem envbox">**Lemma.**
Hitting sets:
Let $S$ be a collection of  $m$ sets of size $\ge k$ over
$V=[n]$.
Then with high probability in $m$, a uniformly random subset
$T\subset V$ of size  $\Omega((n/k) \log m)$ intersects all sets
in $S$ non-trivially. 
</div>

<div class="thm envbox">**Theorem.**
There is an $n^{(3+\omega)/2 + o(1)}$ time algorithm for APSP.
even in directed graphs.
</div>
<div class="pf envbox">**Proof.**
Take a random set $S$ of size $(n/k)\log m$. It hits all paths of
length at least $k$ by the above lemmma.

Run BFS into and out of each vertex $t\in S$ so that we get  $d(u,t), d(t,u)$ for all $u$ in the graph.
Then compute $d(u,v)$ for  $u,v$ which are distance at least
$k$ appart as $\min_{t\in S} d(u,t)+d(t,v)$.

Balance this against the $kn^{\omega}$ algorithm for determining
 $d(u,v)$ in the case that $u,v$ are at most distance $k$
 appart.
</div>

# Siedel's APSP Algorithm

<div class="thm envbox">**Theorem.**
$n^{\omega + o(1)}$ algo for APSP in undirected unweighted graphs
</div>
<div class="pf envbox">**Proof.**
Idea: 

- If $u,v$ are distance  $d$ in $A$, then they are distance
$\ceil{d/2}$ in $A^{2}\lor A$.
- if we also had an algo for "parity of distance between $u,v$"
    then we would have a simple recursive algorithm for APSP
- observe that if $k\in N(j)$ then $|d(i,k) - d(i,j)| \le 1$. So
     $d(i,k)=d(i,j) \iff d(i,k)\equiv d(i,j)\mod 2$.

so it turns out that there is some MM stuff that you can do by
looking at the neighborhood of a vertex to determine the parity
of the distance, based on the above observation. 

</div>

# lecture 4: Zwick's APSP Algorithm

<div class="lem envbox">**Lemma.**
Dijstra's algorithm: 
Given a graph $G$ with non-negative edge weights, there is an
algorithm wtih running time $O(m+n\log n)$ to compute SSSP.
</div>
<div class="pf envbox">**Proof.**
(ok but that's a
little bit overly fancy using Fibonacci heaps, reallly)

The algorithm is as follows: 

- maintain a min-heap of the vertices based on tentative distance
    estimates
- Repeatedly take the closest vertex from the heap, and update
    the distance to all vertices through this vertex

</div>

# lecture 5: weighted APSP

<div class="thm envbox">**Theorem.**
You can also do APSP in weighted undirected graphs pretty fast. 
</div>
<div class="pf envbox">**Proof.**
We again break into cases for short and long paths. 
We again sample a large set and compute distances from everything to and from all vertices of this set. 
Now because stuff is weighted this is a bit more complicated, we
can't just BFS. 

You do SSSP, Johnson's trick (i.e., adding the results of SSSP from an
auxiliary vertex to the weights to get positive weights), and
finally Dijkstra's algorithm. 

For short paths we need a new type of MM: $(\min, +)$-product. 

</div>

<div class="rmk envbox">**Remark.**

$(A\star B) [i,j] = \min_k (A[i,k]+B[k,j])$

$(\min, +)$-product is **actually** equivalent to APSP. 
No "combinatorial algorithm" nonsense. 

However, it turns out you can compute $A\star B$ using fast MM if the entries
are all small integers.
</div>

# lectures 6,7,8

At some later point I might read the remaining lecture notes in
more detail. 
For now I want to really quickly familiarize myself with what 
the problems that people care about in this field are and what
are some of the key results. 

- earliest arrivals
- all-pairs bottleneck paths
- witness matrix


<div class="thm envbox">**Theorem.**
Minimum vertex weight triangle: vertices are given integer weights. 
Find the triangle of minimum weight. 
Can be done in $O(n^{\omega+o(1)})$.
</div>
<div class="pf envbox">**Proof.**

First they give a simple algorithm with sub-optimal running time
based on the min-witness product: 
$$\min \set{k: A[i,k] = B[k,j] = 1}.$$
Can be computed in $n^{2.529}$ time using rectangular MM.

some fancier algorithm does better. 

Seems like some common themes include duplicating vertex set
three times, partitioning into vertex subsets arbitrarily and
doing stuff on those. 

</div>

<div class="thm envbox">**Theorem.**
Min edge-weight triangle is sub-cubically equivalent to APSP.
</div>

<div class="defn envbox">**Definition.**
**graph radius**:
$$\min_v \max_u d(u,v)$$
May also want to find such a minimizing $v$ called a  **center**.
</div>
<div class="thm envbox">**Theorem.**
graph radius subcubically equivalent to APSP.
</div>
<div class="pf envbox">**Proof.**
You can trivially compute the diameter by first computing APSP. 

the other direction is more involved. 
</div>


big open question: is diameter subcubicaly equiv to APSP?

# lecture 9: subgraph isomorphism (SI)

We have a pattern graph with $k\le O(1)$ vertices. 

- induced
- non-induced 

<div class="thm envbox">**Theorem.**
We reduce non-induced SI to induced SI.
</div>
<div class="pf envbox">**Proof.**
Color coding!!
Let $V(H) = h_1,h_2,\ldots, h_k$.
Color vertices of graph with $k$ colors. 
Let $w_1,w_2,\ldots, w_k$ be vertices forming a (not-necessarily
induced) $H$ in $G$.
We will hope that $\chi(w_i) = i$.

After doing the coloring we cut edges between $u,v$ if
$h_{\chi(u)}, h_{\chi(v)}\notin E(H)$.
This turns our non-induced $H$ into an induced $H$. 
The coloring succeeds with constant Pr.
Can be derandomized.

![ink_img004](src/images/ink_img004.png)

</div>

<div class="thm envbox">**Theorem.**
For any $k$-vertex graph $H$,
induced $H$ reduces to induced $K_{k}$.
</div>
<div class="pf envbox">**Proof.**
Color code to make a $k$-partite graph. 
Delete edges internal to the parts.
Flip (i.e., if they are present delete them and if they are no
present add them) edges $u,v$ if $h_{\chi(u)}, h_{\chi(v)}\notin E(H)$.

Now we are looking for a $k$-clique.

</div>

<div class="thm envbox">**Theorem.**
Best $k$-clique algorithm [Nestetnice, Poljak. '85]
Let $k\equiv 0 \mod 3$. Then you can solve $k$-clique in
$n^{k\omega/3}$ time.
</div>
<div class="pf envbox">**Proof.**
We construct a graph on $\frac{k}{3}$-tuples of vertices.
We create a super-node for a $\frac{k}{3}$-tuple iff it is a $\frac{k}{3}$-clique. 
We connect two super-nodes if all edges are present between them.
A triangle of super-nodes is a $k$-clique in the original graph.
Finally, do triangle detection on the super-graph with MM.
</div>

But some $H$ are faster than $k$-clique.

### $k=3$ 

- triangle can be done in $n^{\omega}$ time
- V can be detected in $n^{2}$ time. I think I have an algorithm:

<div class="pf envbox">**Proof.**
  - Make a list of edges
  - Maintain sets $A,B$, where  $A$ is initialized to be an
      edge and $B$ to be empty
      - loop over all the vertices
      - if the vertex is connected to all of $A$ add it to $A$.
      - if the vertex is connected to none of $A$ add it to $B$ 
      - if neither of the above happened, you can just win
          using that vertex
  - At the end of this process, we have sets $A,B$ where $A$ is
      a clique and no vertices in $B$ have any neighbors in $A$.
  - Delete $A$ and repeat the above stuff 
  - should take time $O(n |A|)$ on each iteration, 
  - so total time $O(n^{2})$.
</div>


### $k=4$ 
"Finding Four-Node Subgraphs in Triangle Time" -Williams

- Remark: you can also do stuff based on $m$ (i.e. sparse graphs)
- diamond approach:
  - solve an even harder problem (counting!)
  - Let $A^{2}[i,j]$ denote the number of length-$2$ paths from
      $i$ to $j$.
  - $\sum_{i,j\in E} \binom{A^{2}[i,j]}{2} = 6\cdot \#(K_4) + \#(K_4-e)$
  - somehow we will restrict to a random subset so that diamond
      count is not a multiple of $6$.
  - key lemma: Shwarz-Zippel polynomial identity testing:
      non-zero degree-$d$ multilinear polynomial has Pr at most
      $\frac{1}{2^{d}}$ of being $0$ on a random point.
  - corollary: if you randomly delete half the vertices then with
      probability $2^{-|H|}$ the number of $H$'s in your graph is
      not $0\mod q$ for some prime $q$, assuming that your graph
      had a non-zero number of $H$'s to begin with.
- then they do sparse graphs!
- and some de-randomization
- The best thing people know for $K_4$ is $n^{\omega(1,2,1)}\le n^{3.1}$ (i.e., multiplying $n \times n^{2}$ and $n^{2} \times n$ matrices)

"there is a combinatorial algo for $H$-detection where $H\neq K_k$ is a $k$-vertex pattern with run-time $n^{k-1}$."

> CONJECTURE: For any $k$-vertex pattern $H\neq K_k$, $H$-detection can be done in $K_{k-1}$-detection time.


# coming soon: lectures 10-20