\renewcommand{\O}{\mathcal{O}}
\newcommand{\tilo}{\widetilde{\O}}
\newcommand{\nil}{\varnothing}
\newcommand{\one}{\mathbb{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}
\newcommand{\legendre}[2]{\genfrac{(}{)}{}{}{#1}{#2}}
\newcommand{\EXP}{\mathsf{EXP}}
\newcommand{\PSPACE}{\mathsf{PSPACE}}
\newcommand{\NEXP}{\mathsf{NEXP}}
\newcommand{\PP}{\mathsf{P}}
\newcommand{\NP}{\mathsf{NP}}
\newcommand{\erdos}{Erd\H{o}s}
\newcommand{\pmo}{\set{-1,1}}
\newcommand{\zo}{\set{0,1}}


\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


# Introduction
To introduce the subject of the paper we begin with the
illustrative example of $k$-coloring a random graph.

The random $k$-coloring problem is defined as follows:

> Let $G$ be a random graph (I think they use the "configuration model", i.e., sample a random graph based on the number of edges, but this is basically the same as Erdos Renyi) with average degree $d$. Is it possible properly $k$-color the vertices of $G$?

The answer to this existential problem is, that there is a
threshold at $d=2k\ln k$. 

<div class="prop envbox">**Proposition.**
If $d> (2+\eps)k\ln k$ then the answer
is almost certainly no [TODO: prove].
On the other hand, a second moment method calculation [TODO: do
it] shows that if $d<(2-\eps)k\ln k$ then with probability
$1-o(1)$ [TODO: is the probability expo good?] $G$ is
$k$-colorable.
</div>
<div class="pf envbox">**Proof.**
The proof of existence of a solution is non-constructive: we use
the second moment method.
</div>

A closely related algorithmic question is to give an efficient
algorithm for finding a $k$-coloring of a random graph.
The state-of-the-art for this problem is:
<div class="prop envbox">**Proposition.**
If $d<k\ln k$ then there is an efficient algorithm for
$k$-coloring $G$.
</div>
<div class="pf envbox">**Proof.**
The algorithm is as follows:
```
while not all vertices are colored:
  Let v be a vertex with the fewest remaining choices for its color
  Assign v a random available color
```
[TODO: prove that this works]
</div>

Let the **algorithmic threshold** denote the largest edge density
where we have efficient algorithms for $k$-coloring.
Let the **existence threshold** denote the largest edge density
where we are guaranteed (with good probability) that a
$k$-coloring exists.
Note that there is a gap between the algorithmic threshold and
the existence threshold.
This is a quite general phenomenon across many CSPs; for instance
it also occurs for $k$-SAT.
Achiloptas and Coja-Oghlan's goal in this paper is to provide an
explanation of the gap between these thresholds. They do so by giving a
description of the solution space geometry, and showing that this
space undergoes a dramatic change when we cross the threshold
below which we have efficient algorithms for $k$-coloring.

First we define what we mean by solution space geometry, and
describe qualitatively the phase transition that occurs.
The space of $k$-colorings is simply $[k]^{n}$.
We can think of this as a "landscape".
The **height** in the landscape corresponds to the number of violated
constraints (i.e., monochromatic edges).
The **height** of a path between two colorings is the largest
height at any coloring along the path.
The **distance** between two colorings is the number of vertices
that they assign different colors.
Below the algorithmic threshold they show that there is a "giant
ball" of solutions: this ball is large, and it is easy to move between
solutions, in the sense that starting from a given solution there
is a nearby solution that you can walk to along a low heigh path.
In contrast, above the algorithmic threshold (but still below the
existence threshold so that solutions exist) the solution space
**shatters** and looks like an "error correcting code".
More specifically, the solution space transitions to consisting
of an exponential number of regions, none of which are very
large, and such that the regions are very far apart and separated
by large heights (or "energy barriers").

The most important idea in their analysis is a **transfer principle**.
Roughly speaking this principle says that the view of the
landscape from a random valley is basically the same as the view
from a **planted** valley.
In a **planted** instance of random $k$-coloring instead of
uniformly randomly choosing a graph $G$ of appropriate average
degree, we first fix a coloring $\sigma\in [k]^{n}$ and then
select $G$ from amongst graphs with the appropriate average
degree which are properly colored by $\sigma$.
Intuitively, if a random coloring instance has a very large
number of solutions on average then adding one more solution
won't change its landscape too much.
However, reasoning about the planted model turns out to be *much*
easier than reasoning about the uniform model.

Now we develop these notions more formally. For graph $G$ and
coloring $\sigma\in [k]^{n}$ let $H_G(\sigma)$ count the number
of violated constraints (monochromatic edges) if we color $G$ via
$\sigma$.
Let $S(G)$ denote the set of colorings $\sigma$ with height $H_G(\sigma) = 0$; that is, $S(G)$ is the set of proper $k$-colorings of $G$.
Define the **distance** between two colorings to be the number of
vertices which they assign different colors.
A **cluster** of $G$ is a connected component of $S(G)$, where
two colorings are considered **adjacent** if they have distance
$1$ (differ on a single vertex).
A **region** is a non-empty union of clusters.

Then, the **shattering** phenomenon can be formalized as follows:
<div class="defn envbox">**Definition.**
There exists a partition of $S(G)$ into regions such that:

- The number of regions is at most $\exp(\beta n)$,
- The distance between distinct regions is at least $\zeta n$,
- All paths between distinct regions have height at least $\theta
    n$.
</div>
They show:
<div class="thm envbox">**Theorem.**
Shattering happens right above the algorithm threshold for
$k$-coloring.
</div>

Fix graph $G$ and a proper $k$-coloring $\sigma$ of $G$.
We say that a vertex $v$ is **$f(n)$-rigid** (with respect to $G,\sigma$) if every coloring $\tau\in S(G)\setminus \set{\sigma}$ is distance at least $f(n)$ away from $\sigma$.
Otherwise we say that vertex $v$ is **$f(n)$-loose**.
They show:

<div class="thm envbox">**Theorem.**
Below algorithm threshold all variables are loose (with good pr).
Above algorithm threshold a most variables are rigid.
</div>

# proof sketches

A convincing example of why the transfer principle is reasonable:
Let $M$ be a matrix. Same number of 1s in each row, same number
of 1s in each col.
Then, sample row and then 1 or col and then 1 is the same distr.

We don't actually need each row and each col to have same number
of 1s, just "close to this" in some sense.

To show: 
property holds wupp for random $G,\sigma$
suffices to show holds whp for planted  $G,\sigma$.

important step:
show number of $k$-colorings of random graph concentrates really
well.

**TRANSFER THEOREM**
<div class="thm envbox">**Theorem.**
D: prop that holds for random
E: some prop 

If you can show $\Pr \text{planted}[G,\sigma \text{ has } E | G
\text{ has } D]$ is super close to one then $\Pr
\text{unif}[G,\sigma \text{ has } E]$ is decent.
</div>

## Loose vars below thresh

notion: list-chromatic num

Algo for finding a nearby solution:
awake, asleep, dead vertices.
subcritical branching process!

Then use the list-chromatic thing to handle the dead guys.

## Rigid vars above thresh

<div class="thm envbox">**Theorem.**
There is a big subgraph $G_*$ of $G$ with the property that every
vertex has a lot of neighbors of each other color.
</div>

We use the transfer property to trade for a problem in planted
model.
<div class="lem envbox">**Lemma.**
solve planted version
</div>

<div class="lem envbox">**Lemma.**
bound on expansion
</div>

<div class="cor envbox">**Corollary.**
It's pretty easy to show from the theorem that no vertex in
$G_*$ can have a close coloring.
</div>

## shattering

Now we will prove shattering.
They want some notion of distance that plays nicely with
permuting colors, not exactly sure why but whatever. 

Define $M_{\sigma,\tau}^{ij} = \frac{1}{n} |\sigma^{-1}(i)\cap
\tau^{-1}(j)|$.
Our notion of distance between $\sigma, \tau$ is now gonna be frobenius norm of $M_{\sigma, \tau}$.
Checking for $\sigma = \tau$ it is much bigger than if
$\sigma,\tau$ are "uncorrelated".

<div class="thm envbox">**Theorem.**
shattering.
more specifically, 

there are zero solutions at medium distance from $\sigma$.
There are not so many solutions which are pretty close to
$\sigma$.
</div>
It is clear how this allows us to define regions.

ok so to prove the theorem

step 1: transfer principle ofc.

<div class="lem envbox">**Lemma.**
step 2: prove in planted model.
</div>
<div class="pf envbox">**Proof.**
I think we just compute expectations.
</div>

gg

