\renewcommand{\O}{\mathcal{O}}
\newcommand{\tilo}{\widetilde{\O}}
\newcommand{\nil}{\varnothing}
\newcommand{\one}{\mathbb{1}}
\newcommand{\bigO}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\img}{Img}
\DeclareMathOperator{\polylog}{\text{polylog}}
\DeclareMathOperator{\poly}{\text{poly}}
\newcommand{\st}{\text{ such that }}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\interior}[1]{ {\kern0pt#1}^{\mathrm{o}} }
\newcommand{\mb}{\mathbf}
\newcommand{\partition}{\vdash}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}
\newcommand{\legendre}[2]{\genfrac{(}{)}{}{}{#1}{#2}}
\newcommand{\EXP}{\mathsf{EXP}}
\newcommand{\PSPACE}{\mathsf{PSPACE}}
\newcommand{\NEXP}{\mathsf{NEXP}}
\newcommand{\PP}{\mathsf{P}}
\newcommand{\NP}{\mathsf{NP}}
\newcommand{\erdos}{Erd\H{o}s}
\newcommand{\pmo}{\set{-1,1}}
\newcommand{\zo}{\set{0,1}}


\newcommand{\setof}[2]{\left\{ #1\; : \;#2 \right\}}
\newcommand{\set}[1]{\left\{ #1\right\}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\contr}{\[ \Rightarrow\!\Leftarrow \]}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\ang}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\left| #1 \right|}


# linearity 

Is it the case that 
$$ f(x) = \chi_S(x)? $$ 

$$ f(x)f(y)f(x\oplus y) = f(0)? $$ 

Analysis:
$f(0)f*f*f(0) \ge 1-2\eps.$
Thus 
$$ \max_S |\hat{f}(S)| \ge 1-2\eps. $$ 
Which means that $f$ is close to some character.

# direct product
PROPERTY: 
If $\exists f_1,\ldots, f_k$ such that 
$$ g(x_1,\ldots, x_k) = (f_1(x_1), \ldots, f_k(x_k)) $$ 
then we say that $g$ is a direct product. We want a test to
distinguish between functions which are close to direct products
and functions which are far from direct products.

So here's the test. I think it was originally introduced by Dinur.

1. Sample $x$ uniformly.
2. Sample a set $A$ of coordinates by including each coordinate
   in $A$ with probability $3/4$. 
3. Sample $y$ as follows: for $i\in A$, just set $y_i = x_i$. For
    $i\notin A$ sample  $y_i$ uniformly.
4. Accept iff $g(y)_A = g(x)_A$.

Here is my attempt at analyzing this (I haven't read the paper
yet).
I think if we can prove the following lemma, then it is gg:
<div class="lem envbox">**Lemma.**
$\forall  i\in[k],$ 
 $$ \Pr_{x,y}[g_i(x)=g_i(y)\mid x_i = y_i] \ge 1-100\eps. $$ 
</div>
<div class="pf envbox">**Proof.**
Let $B_1,B_2,B_3,B_4$ be a partition of $[k]\setminus\set{i}$. 
Sample $x^{(0)}$ uniformly randomly.
Sample $x^{(1)}$ by starting with $x^{(0)}$ and then resampling
the coordinates from $B_1$.
Sample $x^{(2)}$ by starting from $x^{(1)}$ and then resampling
the coordinates from $B_2$.
Similar for $x^{(3)}, x^{(4)}$.

Yay coupling. 

Ok so here's the story. 

These $x^{(i)}$'s are all marginally uniform random points.
Hence, by union bound there is at most $8\eps$ chance that any of
them are bad. 
Assuming they are all good, my coupling thing makes it so that
if we compare adjacent ones, i.e., $x^{(i)}$ and $x^{(i+1)}$ then
they are very likely to agree. 

But at the end of the process we have $x^{(0)}$ and $x^{(4)}$
being completely unrelated things, except that they agree in bit $i$.
But by this chain of reasoning we see that $g_i(x^{(0)})$ and
$g_i(x^{(4)})$ are actually quite likely to agree.
In otherwords, we have shown that $g$ is pretty consistent in the
value of $g_i(x)$  when  $x_i$ is held constant. So there is a
natural way to define the direct product and it should work.

</div>

# direct sum

There are more efficient tests for direct sum: this one uses a
large number of queries. But it's quite simple to analyze, so
that's nice. It's by Dinur et at. 

I don't quite parse what they have written. But here's something
that makes sense. 

Let $a^i_b$ denote vector  $a$ except with the $i$-th coordinate
replaced with $b_i$.

Define 
$$ f^{a}_i(b) = f(a^{i}_b), $$ 
and then define
$$ f^a(b) = \bigoplus_i f(a^i_b). $$ 

The test is as follows:

- Sample random $a,b$.
- Test if 
$$ f^a(b) = f(b). $$ 

That is, test something like this:

$$ f(b_1,b_2,b_3) = f(a_1,a_2,b_3) + f(a_1,b_2,a_3) + f(b_1,a_2,a_3).$$ 

If the test passes with decent probability, then by choosing
appropriate $a$ we find that $f^{a}$ is a good approximation for
$f$. 
But $f^{a}$ is of course a direct sum function, so we win: $f$ is
close to a direct sum.

